In Layoutlmv1, image embeddings are only added in the fine-tuning stage. In Layoutlmv2, the image embeddings are integrated in the pre-training stage where the transformer can learn the visual information. Since the 
image embedding does not contain the positional information, 1-D positional embeddings are added to these token image embeddings along with the segment embedding. Padding tokens are also added in the text embedding to 
make the inputs of same length which is mentioned in v2. Along with MVLM, two other objectives are also used while pretraining. They are TIA (Text image allignment) and TIM(Text image matching).  
In TIA, some token's image regions are covered and the model tries to predict wheather each token is covered or not. In TIM, the models is trained to predict wheather the image and text are from the same document page. Negative samples are also 
induced by replacing an image from another document.   
Unlike v1, which uses absolute 2-D postions, v2 uses relative position embedding for better contextual learning. Along with testing the model on documnet understanding (form, recept understanding), model was also tested on
document vitual question answering.

Layoutlmv3 completely eleminated the use of CNN. Instead the images are represented with linear projection features of image patches. The image is split into patches and projected into D dimensions and flattened them into 
a sequence of vectors. MIM (Masked Image Modelling) and WPA (Word patch allignment) objectives were used along with masked language modelling (MLM). 
Similar to MLM, in MIM, we randomly mask some percentage of the image tokens and the model learns to reconstruct these masked image tokens. In WPA, the model assigns alligned/unalligned label to the unmasked words. The word token is labelled alligned when its corresponding image token is also unmasked and vice versa. This lets the model to learn between the text and its image. 
Similar to v2, v3 is also fine tuned for VQA. v3 is also fine-tuned for vision task which aims to detect the layouts of the documents through object detection. 
